-- This file reads the dataset generated by genreateEncoderDataset.lua and
-- trains an encoder net that learns to map an image X to a noise vector Z with 
-- generated images and an attribute vector Y using real images.

require 'image'
require 'nn'
require 'optim'
torch.setdefaulttensortype('torch.FloatTensor')

local function getParameters()
  local opt = {
        name = 'encoder_c_celeba_Anet',
        batchSize = 64,
        outputPath= '././checkpoints/',        -- path used to store the encoder network
        genDatasetPath = '././celebA/generatedDataset/', -- folder where the dataset of generated images is stored (not the file itself)
        realDatasetPath = '././celebA/', -- folder where the dataset of real images is stored
        split = 0.66,           -- split between train and test (i.e 0.66 -> 66% train, 33% test)
        nConvLayers = 4,        -- # of convolutional layers on the net
        nf = 32,                -- #  of filters in hidden layer
        nEpochs = 15,           -- #  of epochs
        lr = 0.0001,            -- initial learning rate for adam
        beta1 = 0.1,            -- momentum term of adam
        display = 1,            -- display 1= train and test error, 2 = error + batches images, 0 = false
        gpu = 1                 -- gpu = 0 is CPU mode. gpu=X is GPU mode on GPU X
        
  }
  
  for k,v in pairs(opt) do opt[k] = tonumber(os.getenv(k)) or os.getenv(k) or opt[k] end
  
  if opt.display then require 'display' end
  
  return opt
end

local function readGeneratedDataset(path)
-- There's expected to find in path a file named groundtruth.dmp
-- which contains the image paths / image tensors and Z and Y input vectors.
    local X
    local data = torch.load(path..'groundtruth.dmp')
    -- We apply a narrow because for CelebA two datasets don't fit in 16 GB of RAM
    -- Therefore, we use half generated images dataset and half real images dataset.
    data.Z = data.Z:narrow(1,1,math.ceil(data.Z:size(1)/2))
    data.Y = data.Y:narrow(1,1,math.ceil(data.Z:size(1)/2))
    local Z = data.Z
    local Y = data.Y
    
    if data.storeAsTensor then
        data.X = data.X:narrow(1,1,math.ceil(data.Z:size(1)/2))
        X = data.X
        assert(Z:size(1)==X:size(1) and Y:size(1)==X:size(1), "groundtruth.dmp is corrupted, number of images and Z and Y vectors is not equal. Create the dataset again.")
    else
        assert(Z:size(1)==#data.imNames and Y:size(1)==#data.imNames, "groundtruth.dmp is corrupted, number of images and Z and Y vectors is not equal. Create the dataset again.")
        
        -- Load images
        local tmp = image.load(data.relativePath..data.imNames[1])
        X = torch.Tensor(#data.imNames, data.imSize[1], data.imSize[2], data.imSize[3])
        X[{{1},{},{},{}}] = tmp
        
        for i=2,#data.imNames do
            X[{{i},{},{},{}}] = image.load(data.relativePath..data.imNames[i])
        end
    end

    return X, Z, Y
end

local function readRealDataset(path)
-- There's expected to find in path a file named images.dmp
-- which contains the images X and attribute vectors Y.

  local X = torch.load(path..'images.dmp')
  print(('Done. Loaded %.2f GB (%d images).'):format((4*X:size(1)*X:size(2)*X:size(3)*X:size(4))/2^30, X:size(1)))
  
  -- Check images are in range [-1, 1]
  if X:min() >= 0 then 
      X:mul(2):add(-1) -- make it [0, 1] -> [-1, 1] 
  end
  
  local Y = torch.load(path..'imLabels.dmp')
  
  -- Hard coded parameters for CelebA
  --X = X:narrow(1,1,202599-19961)
  --Y = Y:narrow(1,1,202599-19961)
  X = X:narrow(1,1,101299)
  Y = Y:narrow(1,1,101299)
  
  
  return X, Y
end

local function splitTrainTestGenerated(x, z, y, split)
    local xTrain, zTrain, yTrain, xTest, zTest, yTest
    
    local nSamples = x:size(1)
    local splitInd = torch.floor(split*nSamples)
    
    xTrain = x[{{1,splitInd},{},{},{}}]
    zTrain = z[{{1,splitInd},{},{},{}}]
    yTrain = y[{{1,splitInd},{}}]
    
    xTest = x[{{splitInd+1,nSamples},{},{},{}}]
    zTest = z[{{splitInd+1,nSamples},{},{},{}}]
    yTest = y[{{splitInd+1,nSamples},{}}]
    
    return xTrain, zTrain, yTrain, xTest, zTest, yTest
end

local function splitTrainTestReal(x, y, split)
    local xTrain, yTrain, xTest, yTest
    
    local nSamples = x:size(1)
    local splitInd = torch.floor(split*nSamples)
    
    xTrain = x[{{1,splitInd},{},{},{}}]
    yTrain = y[{{1,splitInd},{}}]
    
    xTest = x[{{splitInd+1,nSamples},{},{},{}}]
    yTest = y[{{splitInd+1,nSamples},{}}]
    
    return xTrain, yTrain, xTest, yTest
end

local function getEncoderVAE_GAN(sample, nFiltersBase, Zsz, Ysz, nConvLayers)
  -- Encoder architecture taken from Autoencoding beyond pixels using a learned similarity metric (VAE/GAN hybrid)
  -- Zsz: size of the output vector Z
  -- Ysz: size of the output vector Y
  
  -- Sample is used to know the dimensionality of the data. 
  -- For convolutional layers we are only interested in the third dimension (RGB or grayscale)
    local inputSize = sample:size(1)
    local encoder = nn.Sequential()
    -- Assuming nFiltersBase = 64, nConvLayers = 3
    -- 1st Conv layer: 5×5 64 conv. ↓, BNorm, ReLU
    --           Data: 32x32 -> 16x16
    encoder:add(nn.SpatialConvolution(inputSize, nFiltersBase, 5, 5, 2, 2, 2, 2))
    encoder:add(nn.SpatialBatchNormalization(nFiltersBase))
    encoder:add(nn.ReLU(true))
    
    -- After 1st conv layer, split network in two: one module outputs Z, the other Y.
    -- Both share the same architecture, but do not share parameters.
    local ct = nn.ConcatTable()
    
    for i=1,2 do -- repeat 2 times, one for each module (Z and Y)
        local ctModule = nn.Sequential()
        -- 2nd Conv layer: 5×5 128 conv. ↓, BNorm, ReLU
        --           Data: 16x16 -> 8x8
        -- 3rd Conv layer: 5×5 256 conv. ↓, BNorm, ReLU
        --           Data: 8x8 -> 4x4
        local nFilters = nFiltersBase
        for j=2,nConvLayers do
            ctModule:add(nn.SpatialConvolution(nFilters, nFilters*2, 5, 5, 2, 2, 2, 2))
            ctModule:add(nn.SpatialBatchNormalization(nFilters*2))
            ctModule:add(nn.ReLU(true))
            nFilters = nFilters * 2
        end
        
        
         -- 4th FC layer: 2048 fully-connected
        --         Data: 4x4 -> 16
        ctModule:add(nn.View(-1):setNumInputDims(3)) -- reshape data to 2d tensor (samples x the rest)
        -- Assuming squared images and conv layers configuration (kernel, stride and padding) is not changed:
        --nFilterFC = (imageSize/2^nConvLayers)²*nFiltersLastConvNet
        local inputFilterFC = (sample:size(2)/2^nConvLayers)^2*nFilters
        ctModule:add(nn.Linear(inputFilterFC, inputFilterFC)) 
        ctModule:add(nn.BatchNormalization(inputFilterFC))
        ctModule:add(nn.ReLU(true))
        local outputSize
        if i==1 then outputSize = Zsz else outputSize = Ysz end
        ctModule:add(nn.Linear(inputFilterFC, outputSize))
        --ctModule:add(nn.Tanh()) 
        
        ct:add(ctModule)
    end
    
    encoder:add(ct)
    
    local mse = nn.MSECriterion()
    local criterion = nn.ParallelCriterion():add(mse):add(mse)
    
    return encoder, criterion
end

local function getEncoderALI(sample, nFiltersBase, Zsz, Ysz)
  -- Encoder architecture taken from Autoencoding beyond pixels using a learned similarity metric (VAE/GAN hybrid)
  -- Zsz: size of the output vector Z
  -- Ysz: size of the output vector Y
  
  -- Sample is used to know the dimensionality of the data. 
  -- For convolutional layers we are only interested in the third dimension (RGB or grayscale)
    local inputSize = sample:size(1)
    local outputSize
    local encoder = nn.Sequential()
    --3 x 64 x 64
    encoder:add(nn.SpatialConvolution(inputSize, 64, 2, 2, 1, 1, 0, 0))
    encoder:add(nn.SpatialBatchNormalization(64))
    encoder:add(nn.LeakyReLU(0.2, true))
    --w = 64; k=2; s=1; p=0;
    --math.floor((w+2*p-k)/s+1)
    
    -- After 1st conv layer, split network in two: one module outputs Z, the other Y.
    -- Both share the same architecture, but do not share parameters.
    local ct = nn.ConcatTable()
    
    for i=1,2 do -- repeat 2 times, one for each module (Z and Y)
        local ctModule = nn.Sequential()
        -- 64 x 63 x 63
        ctModule:add(nn.SpatialConvolution(64, 128, 7, 7, 2, 2, 0, 0))
        ctModule:add(nn.SpatialBatchNormalization(128))
        ctModule:add(nn.LeakyReLU(0.2, true))
        -- 128 x 29 x 29 
        ctModule:add(nn.SpatialConvolution(128, 256, 5, 5, 2, 2, 0, 0))
        ctModule:add(nn.SpatialBatchNormalization(256))
        ctModule:add(nn.LeakyReLU(0.2, true))
        -- 256 x 13 x 13 
        ctModule:add(nn.SpatialConvolution(256, 256, 7, 7, 2, 2, 0, 0))
        ctModule:add(nn.SpatialBatchNormalization(256))
        ctModule:add(nn.LeakyReLU(0.2, true))
        -- 256 x 4 x 4
        ctModule:add(nn.SpatialConvolution(256, 512, 4, 4, 1, 1, 0, 0))
        ctModule:add(nn.SpatialBatchNormalization(512))
        ctModule:add(nn.LeakyReLU(0.2, true))
        -- 512 x 1 x 1
        if i==1 then outputSize = Zsz else outputSize = Ysz end
        ctModule:add(nn.SpatialConvolution(512, outputSize, 1, 1, 1, 1, 0, 0))
        -- outputSize x 1 x 1
        ctModule:add(nn.View(-1):setNumInputDims(3))
        -- outputSize
        
        ct:add(ctModule)
    end
    
    encoder:add(ct)
    
    local mse = nn.MSECriterion()
    local criterion = nn.ParallelCriterion():add(mse):add(mse)
    
    return encoder, criterion
end

local function assignBatchesGenerated(batchX, batchZ, batchY, x, z, y, tmpX, tmpZ, tmpY, batch, batchSize, shuffle)
    
    data_tm:reset(); data_tm:resume()
    
    batchX:copy(x:index(1, shuffle[{{batch,batch+batchSize-1}}]:long()))
    batchZ:copy(z:index(1, shuffle[{{batch,batch+batchSize-1}}]:long()))
    batchY:copy(y:index(1, shuffle[{{batch,batch+batchSize-1}}]:long()))
    
    data_tm:stop()
    
    return batchX, batchZ, batchY
end

local function assignBatchesReal(batchX, batchY, x, y, batch, batchSize, shuffle)
    
    data_tm:resume()
    
    batchX:copy(x:index(1, shuffle[{{batch,batch+batchSize-1}}]:long()))
    batchY:copy(y:index(1, shuffle[{{batch,batch+batchSize-1}}]:long()))
    
    data_tm:stop()
    
    return batchX, batchY
end

local function displayConfig(disp, title)
    -- initialize error display configuration
    local errorData, errorDispConfig
    if disp then
        errorData = {}
        errorDispConfig =
          {
            title = 'Encoder error - ' .. title,
            win = 1,
            labels = {'Epoch', 'Train error', 'Test error'},
            ylabel = "Error",
            legend='always'
          }
    end
    return errorData, errorDispConfig
end

function main()

  local opt = getParameters()
  if opt.display then display = require 'display' end
  
  -- Set timers
  local epoch_tm = torch.Timer()
  local tm = torch.Timer()
  data_tm = torch.Timer()

  -- Read generated dataset
  local Xg, Z, Yg
  Xg, Z, Yg = readGeneratedDataset(opt.genDatasetPath)
  
  -- Read real dataset
  local Xr, Yr
  Xr, Yr = readRealDataset(opt.realDatasetPath)
  
  -- Split train and test
  local xTrain, zTrain, yTrain, xTest, zTest, yTest
  -- z --> contain Z vectors    y --> contain Y vectors
  xTrain, zTrain, yTrain, xTest, zTest, yTest = splitTrainTestGenerated(Xg, Z, Yg, opt.split)
  
  local xrTrain, yrTrain, xrTest, yrTest
  xrTrain, yrTrain, xrTest, yrTest = splitTrainTestReal(Xr, Yr, opt.split)

  -- X: #samples x im3 x im2 x im1
  -- Z: #samples x 100 x 1 x 1 
  -- Y: #samples x ny
  
  -- Set network architecture
  local encoder, criterion = getEncoderVAE_GAN(xTrain[1], opt.nf, zTrain:size(2), yTrain:size(2), opt.nConvLayers)
 
  -- Initialize batches
  local batchX = torch.Tensor(opt.batchSize, xTrain:size(2), xTrain:size(3), xTrain:size(4))
  local batchZ = torch.Tensor(opt.batchSize, zTrain:size(2))
  local batchY = torch.Tensor(opt.batchSize, yTrain:size(2))
  
  -- Copy variables to GPU
  if opt.gpu > 0 then
     require 'cunn'
     cutorch.setDevice(opt.gpu)
     batchX = batchX:cuda();  batchZ = batchZ:cuda(); batchY = batchY:cuda();
     
     if pcall(require, 'cudnn') then
        require 'cudnn'
        cudnn.benchmark = true
        cudnn.convert(encoder, cudnn)
     end
     
     encoder:cuda()
     criterion:cuda()
  end
  
  local params, gradParams = encoder:getParameters() -- This has to be always done after cuda call
  
  -- Define optim (general optimizer)
  local errorTrain
  local errorTest
  local function optimFunction(params) -- This function needs to be declared here to avoid using global variables.
      -- reset gradients (gradients are always accumulated, to accommodat batch methods)
      gradParams:zero()
      
      local outputs = encoder:forward(batchX)
      errorTrain = criterion:forward(outputs, {batchZ, batchY})
      local dloss_doutput = criterion:backward(outputs, {batchZ, batchY})
      -- We divide by half the gradients of Y to compensate the fact that Y will be trained twice
      -- as many times as Z (one for generated images and another for real images).
      dloss_doutput[2]:mul(0.5)
      encoder:backward(batchX, dloss_doutput)
      
      return errorTrain, gradParams
  end
  
  local function optimFunctionRealData(params)
      -- Here we update the encoder using real images X and its attributes Y.
      -- No Z is involved. Therefore, we only update the Y part of the encoder.
      gradParams:zero()
      -- batchX must contain real images, not generated ones.
      -- The content of batchZ won't be used.
      local outputs = encoder:forward(batchX)
      errorTrain = criterion:forward(outputs, {batchZ, batchY})
      local dloss_doutput = criterion:backward(outputs, {batchZ, batchY})
      -- As we only want to update Y and not Z, we put the gradients w.r.t Z to 0.
      dloss_doutput[1]:zero()
      -- We divide by half the gradients of Y to compensate the fact that Y will be trained twice
      -- as many times as Z (one for generated images and another for real images).
      dloss_doutput[2]:mul(0.5)
      encoder:backward(batchX, dloss_doutput)
      
      return errorTrain, gradParams
  end
  
  local optimState = {
     learningRate = opt.lr,
     beta1 = opt.beta1,
  }
  
  local nTrainSamples = xTrain:size(1) --xTrain:size(1) == xrTrain:size(1)
  local nTestSamples = xTest:size(1)
  
  -- Initialize display configuration (if enabled)
  local errorData, errorDispConfig = displayConfig(opt.display, opt.name)
  paths.mkdir(opt.outputPath)
  
  -- Train network
  local batchIterations = 0 -- for display purposes only
  for epoch = 1, opt.nEpochs do
      epoch_tm:reset()
      local shuffle = torch.randperm(nTrainSamples)
      for batch = 1, nTrainSamples-opt.batchSize+1, opt.batchSize  do
          tm:reset()
          -- Assign batches (generated data)
          --[[local splitInd = math.min(batch+opt.batchSize, nTrainSamples)
          batchX:copy(xTrain[{{batch,splitInd}}])
          batchY:copy(yTrain[{{batch,splitInd}}])--]]
                   
          batchX, batchZ, batchY = assignBatchesGenerated(batchX, batchZ, batchY, xTrain, zTrain, yTrain, batch, opt.batchSize, shuffle)
          
          if opt.display == 2 and batchIterations % 20 == 0 then
              display.image(image.toDisplayTensor(batchX,0,torch.round(math.sqrt(opt.batchSize))), {win=2, title='Train mini-batch'})
          end
          
          -- Update network (generated data)
          optim.adam(optimFunction, params, optimState)
          
          -- Assign batches (real data)
          batchX, batchY = assignBatchesReal(batchX, batchY, xTrain, yTrain, batch, opt.batchSize, shuffle)
          
          -- Update network (real data)
          optim.adam(optimFunctionRealData, params, optimState)           
          
          -- Display train and test error
          if opt.display and batchIterations % 20 == 0 then
              -- Test error (generated images only)
              batchX, batchZ, batchY = assignBatchesGenerated(batchX, batchZ, batchY, xTest, zTest, yTest, tmpX, tmpZ, tmpY, torch.random(1,nTestSamples-opt.batchSize+1), opt.batchSize, torch.randperm(nTestSamples))
              local outputs = encoder:forward(batchX)
              errorTest = criterion:forward(outputs, {batchZ, batchY})
              table.insert(errorData,
              {
                batchIterations/math.ceil(nTrainSamples / opt.batchSize), -- x-axis
                errorTrain, -- y-axis for label1
                errorTest -- y-axis for label2
              })
              display.plot(errorData, errorDispConfig)
              if opt.display == 2 then
                  display.image(image.toDisplayTensor(batchX,0,torch.round(math.sqrt(opt.batchSize))), {win=3, title='Test mini-batch'})
              end
          end
          
          -- Verbose
          if ((batch-1) / opt.batchSize) % 1 == 0 then
             print(('Epoch: [%d][%4d / %4d]  Error (train): %.4f  Error (test): %.4f  '
                       .. '  Time: %.3f s  Data time: %.3f s'):format(
                     epoch, ((batch-1) / opt.batchSize),
                     math.ceil(nTrainSamples / opt.batchSize),
                     errorTrain and errorTrain or -1,
                     errorTest and errorTest or -1,
                     tm:time().real, data_tm:time().real))
         end
         batchIterations = batchIterations + 1
      end
      print(('End of epoch %d / %d \t Time Taken: %.3f s'):format(
            epoch, opt.nEpochs, epoch_tm:time().real))
            
      -- Store network
      --torch.save(opt.outputPath .. opt.name .. '_' .. epoch .. 'epochs.t7', encoder:clearState())
      --torch.save('checkpoints/' .. opt.name .. '_error.t7', errorData)
  end
  
  if opt.poweroff > 0 then os.execute("poweroff") end
  
end

main()